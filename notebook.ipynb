{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhy don't scientists trust atoms? Because they make up everything! üòÇ\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()\n",
    "model = Ollama(model=MODEL)\n",
    "model.invoke(\"tell a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why don't scientists trust atoms? Because they make up everything! üòÇ\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"Tell a joke\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 0}, page_content='Low-light image enhancement of permanently shadowed\\nlunar regions with physics-based machine learning\\nBen Moseley\\nUniversity of OxfordValentin Bickel\\nETH Zurich/ MPS GoettingenIgnacio G. L√≥pez-Francos\\nNASA Ames Research Center\\nLoveneesh Rana\\nUniversity of LuxembourgMiguel Olivares-Mendez\\nUniversity of LuxembourgDennis Wingo\\nSkycorp Inc.\\nAllison Zuniga\\nNASA Ames Research CenterNuno Subtil\\nNVIDIA\\nAbstract\\nFinding water(-ice) on the lunar surface is key to enabling a sustainable human presence\\non the Moon and beyond. There is evidence that water-ice is abundant in and around\\nthe Moon‚Äôs Permanently Shadowed Regions (PSRs), however, direct visual detection\\nhas not yet been possible. Surface ice or related physical features could potentially\\nbe directly detected from high-resolution optical imagery, but, due to the extremely\\nlow-light conditions in these areas, high levels of sensor and photon noise make this\\nvery challenging. In this work we generate high-resolution, low-noise optical images\\nover lunar PSRs by using two physics-based deep neural networks to model and remove\\nCCD-related and photon noise in existing low-light optical imagery, potentially paving\\nthe way for a direct water-ice detection method.\\n1 Introduction\\nIn recent years the international space community has gained signiÔ¨Åcant momentum for continuing the\\nexploration of the Moon and establishing a sustainable presence, with multiple national space agencies and\\nprivate organizations working towards this goal (Neal et al., 2009). A critical objective of these efforts is to\\nidentify potential lunar water resources, because water is a key element for life support and a source of fuel\\n(Hydrogen and Oxygen) (ESA, 2019). Lunar water is also a primary target for scientiÔ¨Åc investigations,\\nsuch as studies of the evolution of our solar system.\\nRecent missions have found evidence that suggests water(-ice) exists on the Moon and could be abundant.\\nAmongst others, observations from Chandrayaan-1 (Li et al. (2018); Pieters et al. (2009)), the Lunar\\nReconnaissance Orbiter (LRO, Hayne et al. (2015); Fisher et al. (2017); Hayne et al. (2020)), the Lunar\\nCrater Observation and Sensing Satellite (LCROSS, Colaprete et al. (2010)), and the airborne SOFIA\\nobservatory (Honniball et al. (2020)) indicate that water likely exists in and around topographic depressions\\nat the lunar poles, mainly in the so-called Permanently Shadowed Regions (PSRs).\\nWhilst these observations are a signiÔ¨Åcant step forward, the direct detection of (surface) water-ice or\\nrelated physical features in PSRs has not yet been possible. The presence of bright ice could potentially be\\ndirectly detected using high-resolution optical imagery. Currently the best imagery of the polar regions is\\nproduced by LRO‚Äôs Narrow Angle Camera (NAC) with a nominal (polar) spatial resolution of 1 to 2 m/pixel\\n(Robinson et al., 2010; Humm et al., 2016). However, due to the extremely low lighting conditions over\\nPSRs, Charge-Coupled Device (CCD) sensor-related and photon noise dominate these images, strongly\\nlimiting our ability to make meaningful observations.\\nIn this work we generate high-resolution, low-noise images of PSRs by training two deep neural networks\\nto model and remove CCD and photon noise in these images, signiÔ¨Åcantly improving their quality. We\\nname our method Hyper-effective nOise Removal U-net Software (HORUS).\\nThird Workshop on Machine Learning and the Physical Sciences (NeurIPS 2020), Vancouver, Canada.'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 1}, page_content='Figure 1: Performance of HORUS over 3 example PSRs, compared to the current calibration routine\\n(ISIS) (Humm et al., 2016; Gaddis et al., 1997). Colour bar shows the estimated surface signal Sfor the\\nDestripeNet+PhotonNet plot. Raw/ISIS image credits to LROC/ASU/GSFC.\\n2 Methods\\nThe LRO NAC takes 12-bit panchromatic optical images and consists of two nominally identical, 5064-\\npixel line-scan CCD cameras oriented perpendicular to the direction of Ô¨Çight of the satellite (Robinson\\net al., 2010). Each camera has two operating modes, ‚Äúregular‚Äù and ‚Äúsummed‚Äù, where the summed mode\\nsums adjacent pixels and is typically used to maximize the signal received over poorly lit regions of the\\nMoon. Over 10 years of operation the LRO NAC has captured over 2 million images, covering the entire\\nsurface multiple times over.\\nFigure 1 shows example raw NAC images captured over 3 example PSRs at the lunar south pole. Whilst\\nthey receive no direct sunlight, PSRs can be illuminated by indirect light scattered from their surroundings.\\nIn this extremely low-light setting NAC images are dominated by CCD noise and photon noise, which\\nis due to the inherent randomness of photons arriving at the CCD. Informed by the work of Humm et al.\\n(2016) and standard CCD theory (Chromey, 2016), in this work we assume that this noise can be described\\nby the following physical model,\\nI=S+Np+Nd+Nb+Nr, (1)\\nwhere Iis the raw image detector count, Sis the mean photon signal, Npis the stochastic photon noise,\\nwhich obeys a Poisson distribution with an expected value equal to S,Ndis stochastic dark current noise,\\nwhich is generated by thermally-activated electrons in the CCD and obeys a Poisson distribution, Nbis\\nthe dark bias, which is deterministic and due to a voltage offset which varies pixel-to-pixel, and Nris the\\nreadout noise. Of these noise sources, the photon noise and dark bias are considered the most dominant\\n(Humm et al., 2016) and are the focus of this study.\\nThe current calibration routine for the NAC images (ISIS, or Integrated Software for Imagers and Spec-\\ntrometers (Humm et al., 2016; Gaddis et al., 1997)) only attempts to remove the dark bias from each\\nimage, by subtracting a cyclic trend from its set of 60 masked pixels and a single library dark calibration\\nframe. Examples images after this calibration are shown in Figure 1: we observe that photon noise and a\\nsigniÔ¨Åcant amount of dark bias (residual stripes) are still present.\\nIn this work we remove both photon and dark bias noise from the images by using two deep neural\\nnetworks, collectively named HORUS. The Ô¨Årst network is called DestripeNet and it models the dark\\nbias. The input to the network is the camera metadata available at the time of image capture (listed in\\nFigure 2) and its output is a prediction of the dark bias frame. We train this network using a set of 100,000+\\ncalibration images as labels, taken on the non-illuminated side of the Moon, where negligible photon signal\\nis present. We use a convolutional decoder network design, shown in Figure 2 (top). Thus, this network\\nlearns a time-varying physics-based model of the CCD sensor.\\nThe second network is called PhotonNet and it models photon noise in the images. The input to the network\\nis the image after DestripeNet has been applied and its output is a prediction of the mean surface signal S.\\nThis network is trained using synthetically generated noisy images by taking 1 million randomly selected\\n2'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 2}, page_content='Figure 2: Physics-based HORUS denoising approach and noise model. Top: DestripeNet and PhotonNet\\nnetworks. DestripeNet uses a convolutional decoder design with 13 layers and 512 hidden channels to\\nmodel the dark bias frame Nb. PhotonNet uses a U-Net design with 4 down-sampling steps to remove\\nphoton noise Np. Both networks are trained using an L2 loss function. PhotonNet operates on images\\npatches, whilst DestripeNet predicts by image line. Bottom: noise model used in this work, showing an\\nexample dark calibration frame ( Nb). Image credits to LROC/ASU/GSFC.\\n40 60 80 100 120\\nSolar incidence angle / secondary incidence angle (degrees)0200400600800Number of NAC imagesTraining images\\n0.00.20.40.60.81.0\\nNormalised secondary illumination fluxShackleton\\nFaustini\\nCabeus\\nScott E\\n0 15 30 45 60 75\\nx (km)0\\n15\\n30\\n45\\n60y (km)\\n0 20 40 60 80\\nMean photon signal, S0.00.20.40.60.81.0Normalised count\\nFigure 3: 3D ray tracing and training distribution matching. Left: histogram of the secondary incidence\\nangle of scattered rays for 4 PSRs simulated using 3D ray tracing, compared to the distribution of solar\\nincidence angles in the training set for PhotonNet. Middle: A render of the secondary illumination radiance\\nfor the Shackleton crater PSR; here the crater rim is a strong source of scattered light. Right: distribution of\\nphoton counts over the re-scaled bright images used to train PhotonNet. Typically, PSRs have raw detector\\ncounts Iin the range 0-50 and we re-scale to this level.\\nbright, sunlit patches of the Moon, re-scaling them to low-lighting photon counts, adding Poisson noise to\\nthem, and then learning to remove this noise. We use a U-Net design (Ronneberger et al., 2015), similar to\\nother low-light denoising approaches (Chen et al., 2018).\\nAn important consideration is that the predictions of PhotonNet will be biased towards the distribution of\\nimages it has been trained on. Whilst it is trained on images with direct solar illumination, it is intended\\nfor use on images of PSRs, which are illuminated by secondary (backscattered) light only. In order to\\nmatch the training data to the test-time data as best we can, we carry out 3D ray tracing of 4 PSRs at the\\nsouth pole and plot the expected distribution of secondary illumination angle under typical illumination\\nconditions. We then match the distribution of solar incidence angles in our training images for PhotonNet\\nto this distribution. Ray tracing is performed using 30 m/pixel spatial resolution LRO Lunar Orbiter Laser\\nAltimeter elevation data (Riris et al., 2017) and a Lambertian bidirectional reÔ¨Çection function. Finally, to\\naccount for possible differences in the two cameras and two operating modes, we train separate networks\\nfor each possible conÔ¨Åguration.\\n3 Results and Discussion\\nHistograms of the incidence angle of scattered rays from ray tracing are shown in Figure 3. We Ô¨Ånd that\\nthese rays typically have incidence angles in the range 60 to 100 degrees (normal to the Moon‚Äôs sphere)\\nand our matched distribution of solar incidence angles in the training set is shown.\\nExample real-data test images of PSRs after training DestripeNet and PhotonNet are shown in Figure 1.\\nWe Ô¨Ånd that DestripeNet removes more of the dark bias ‚Äústripes‚Äù in the images compared to the current\\ncalibration routine, and PhotonNet signiÔ¨Åcantly improves the image quality, removing the ‚Äúgraininess‚Äù of\\nthe photon noise. We generate a test set of 100,000 unseen synthetic noisy images by adding samples of\\n3'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 3}, page_content='Baseline ISIS DestripeNet DestripeNet+PhotonNet\\nMode Camera L1 / Peak signal-to-noise ratio (PSNR) / Structural similarity index measure (SSIM)\\nNormal Left 8.12 / 29.89 / 0.56 3.79 / 35.23 / 0.79 3.67 / 35.63 / 0.80 1.08 /47.49 /0.98\\nRight 9.23 / 29.18 / 0.53 3.79 / 35.31 / 0.79 3.70 / 35.62 / 0.80 1.09 /47.62 /0.98\\nSummed Left 7.75 / 30.33 / 0.59 3.73 / 35.37 / 0.80 3.63 / 35.75 / 0.81 1.20 /46.76 /0.97\\nRight 9.19 / 29.29 / 0.54 3.79 / 35.39 / 0.80 3.72 / 35.65 / 0.81 1.25 /46.89 /0.97\\nTable 1: Test set performance of HORUS. We compare to a baseline approach, where simply the mean\\nmasked pixel value is subtracted from the raw image, and the current calibration routine (ISIS) (Humm\\net al., 2016; Gaddis et al., 1997). The average performance over all images is reported.\\nFigure 4: Ground truth veriÔ¨Åcation. The comparison of a non-HORUS sunlit and a HORUS shadowed\\nimage (contrast and brightness-adapted) in a temporarily shadowed region indicates that our denoising\\ndoes not remove or add any physical features. Non-HORUS image credit to LROC/ASU/GSFC.\\ndark calibration images and photon noise to re-scaled sunlit images and report the performance of HORUS\\non this dataset in Table 1. This table shows that HORUS signiÔ¨Åcantly outperforms the current calibration\\nroutine under all tested metrics for these images.\\nA key concern is whether our networks ‚Äúhallucinate‚Äù or remove physical features from the images. To\\nhelp investigate this, we plot a non-HORUS sunlit image of a Temporarily Shadowed Region (TSR) and\\ncompare this to a shadowed image with HORUS applied in Figure 4. Encouragingly, we Ô¨Ånd that HORUS\\ndoes not remove or add any incorrect physical features compared to the sunlit image.\\n4 Conclusion and Further Work\\nWe have shown that it is possible to signiÔ¨Åcantly enhance the quality of extremely low-light images of\\nPSRs on the Moon by using two physics-based deep neural networks to remove CCD and photon noise.\\nOur approach provides high-resolution, low-noise images which could help to directly detect water-ice.\\nWe have run HORUS over 20+ PSRs and future work will analyse these images for surface-ice related\\nsignals, as well as quantitatively assess the performance of HORUS using ground truth TSR images and\\ninvestigate joint training of the networks to further improve performance.\\n5 Broader impact\\nThis work could signiÔ¨Åcantly impact humanity‚Äôs plans for exploration of the lunar poles, helping to enable\\na sustainable, long term presence on the Moon and beyond. First, our work could directly inform robotic\\nscouting missions to the poles, enabling identiÔ¨Åcation of water(-ice)-related science targets and reducing\\nuncertainty when planning traverses into PSRs. Secondly, our work could support the planning and\\nexecution of the upcoming NASA Artemis missions, which will see the Ô¨Årst woman and the next man\\noperating around and potentially within lunar polar PSRs.\\nAcknowledgements. This work was completed as part of the 2020 NASA Frontier Development Lab\\n(FDL). The authors would like to thank Julie Stopar and Nick Estes for insightful comments on the\\nNAC instrument, Eugene D‚ÄôEon for advice on the ray tracing, and all of the FDL reviewers and partners\\n(Luxembourg Space Agency, Google Cloud, Intel AI and NVIDIA).\\nContributions. VTB and BM were involved in the conceptualization of this project. VTB, BM, ILF and\\nLR were involved in the methodology, software, validation, formal analysis, investigation, resources, data\\ncuration, writing, preparation, visualization. VTB, BM, ILF, LR, AZ, DW and MOM were involved in\\nthe review, administration, supervision. ILF, AZ, DW and the FDL management team were involved in\\nfunding acquisition. NS and EDE implemented the 3D ray-tracing.\\n4'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 4}, page_content='References\\nNeal et al. The lunar exploration roadmap. exploring the moon in the 21st century: Themes, goals,\\nobjectives, investigations, and priorities, 2009. LEAG Roadmap , 2009.\\nESA. Esa space resources strategy. ESA Report , 2019.\\nLi et al. Direct evidence of surface exposed water ice in the lunar polar regions. PNAS , 2018.\\nPieters et al. Character and spatial distribution of oh/h2o on the surface of the moon seen by m3 on\\nchandrayaan-1. Science , 2009.\\nHayne et al. Evidence for exposed water ice in the moon‚Äôs south polar regions from lunar reconnaissance\\norbiter ultraviolet albedo and temperature measurements. Icarus , 2015.\\nFisher et al. Evidence for surface water ice in the lunar polar regions using reÔ¨Çectance measurements\\nfrom the lunar orbiter laser altimeter and temperature measurements from the diviner lunar radiometer\\nexperiment. Icarus , 2017.\\nHayne et al. Micro cold traps on the moon. Nature Astronomy , 2020. doi: https://doi.org/10.1038/\\ns41550-020-1198-9.\\nColaprete et al. Detection of water in the lcross ejecta plume. Science , 2010.\\nHonniball et al. Molecular water detected on the sunlit moon by soÔ¨Åa. Nature Astronomy , 2020. doi:\\nhttps://doi.org/10.1038/s41550-020-01222-x.\\nM. S. Robinson, S. M. Brylow, M. Tschimmel, D. Humm, S. J. Lawrence, P. C. Thomas, B. W. Denevi,\\nE. Bowman-Cisneros, J. Zerr, M. A. Ravine, M. A. Caplinger, F. T. Ghaemi, J. A. Schaffner, M. C.\\nMalin, P. Mahanti, A. Bartels, J. Anderson, T. N. Tran, E. M. Eliason, A. S. McEwen, E. Turtle, B. L.\\nJolliff, and H. Hiesinger. Lunar reconnaissance orbiter camera (LROC) instrument overview. Space\\nScience Reviews , 150(1-4):81‚Äì124, jan 2010.\\nD. C. Humm, M. Tschimmel, S. M. Brylow, P. Mahanti, T. N. Tran, S. E. Braden, S. Wiseman, J. Danton,\\nE. M. Eliason, and M. S. Robinson. Flight Calibration of the LROC Narrow Angle Camera. Space\\nScience Reviews , 200(1-4):431‚Äì473, 2016.\\nL. Gaddis, J. Anderson, K. Becker, T. Becker, D. Cook, K. Edwards, E. Eliason, T. Hare, H. Kieffer, E. M.\\nLee, J. Mathews, L. Soderblom, T. Sucharski, J. Torson, A. McEwen, and M. Robinson. An Overview of\\nthe Integrated Software for Imaging Spectrometers (ISIS). In Lunar and Planetary Science Conference ,\\nLunar and Planetary Science Conference, page 387, March 1997.\\nFrederick R. Chromey. To Measure the Sky . Cambridge University Press, oct 2016.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\\nimage segmentation. ArXiv e-prints , may 2015.\\nChen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to See in the Dark. In Proceedings of the\\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 3291‚Äì3300,\\n2018. ISBN 9781538664209.\\nH. Riris, G. Neuman, J. Cavanaugh, X. Sun, P. Liiva, and M. Rodriguez. The Lunar Orbiter Laser\\nAltimeter (LOLA) on NASA‚Äôs Lunar Reconnaissance Orbiter (LRO) mission. In Naoto Kadowaki,\\neditor, International Conference on Space Optics ‚Äî ICSO 2010 , volume 10565, page 77. SPIE, nov\\n2017. ISBN 9781510616196.\\n5')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"NeurIPS_ML4PS_2020_43.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you\n",
      "can't answer, reply \"I dont know\".\n",
      "\n",
      "Context: here is some context\n",
      "Question: here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you\n",
    "can't answer, reply \"I dont know\".\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\" \n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"here is some context\", question=\"here is a question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}},\n",
       " 'required': ['context', 'question']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Shezwaa! üòä Based on the context you provided, your name is indeed Shezwaa. üéâ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"my name is shezwaa\",\n",
    "        \"question\": \"what is my name?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    pages, \n",
    "    embedding= embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 4}, page_content='References\\nNeal et al. The lunar exploration roadmap. exploring the moon in the 21st century: Themes, goals,\\nobjectives, investigations, and priorities, 2009. LEAG Roadmap , 2009.\\nESA. Esa space resources strategy. ESA Report , 2019.\\nLi et al. Direct evidence of surface exposed water ice in the lunar polar regions. PNAS , 2018.\\nPieters et al. Character and spatial distribution of oh/h2o on the surface of the moon seen by m3 on\\nchandrayaan-1. Science , 2009.\\nHayne et al. Evidence for exposed water ice in the moon‚Äôs south polar regions from lunar reconnaissance\\norbiter ultraviolet albedo and temperature measurements. Icarus , 2015.\\nFisher et al. Evidence for surface water ice in the lunar polar regions using reÔ¨Çectance measurements\\nfrom the lunar orbiter laser altimeter and temperature measurements from the diviner lunar radiometer\\nexperiment. Icarus , 2017.\\nHayne et al. Micro cold traps on the moon. Nature Astronomy , 2020. doi: https://doi.org/10.1038/\\ns41550-020-1198-9.\\nColaprete et al. Detection of water in the lcross ejecta plume. Science , 2010.\\nHonniball et al. Molecular water detected on the sunlit moon by soÔ¨Åa. Nature Astronomy , 2020. doi:\\nhttps://doi.org/10.1038/s41550-020-01222-x.\\nM. S. Robinson, S. M. Brylow, M. Tschimmel, D. Humm, S. J. Lawrence, P. C. Thomas, B. W. Denevi,\\nE. Bowman-Cisneros, J. Zerr, M. A. Ravine, M. A. Caplinger, F. T. Ghaemi, J. A. Schaffner, M. C.\\nMalin, P. Mahanti, A. Bartels, J. Anderson, T. N. Tran, E. M. Eliason, A. S. McEwen, E. Turtle, B. L.\\nJolliff, and H. Hiesinger. Lunar reconnaissance orbiter camera (LROC) instrument overview. Space\\nScience Reviews , 150(1-4):81‚Äì124, jan 2010.\\nD. C. Humm, M. Tschimmel, S. M. Brylow, P. Mahanti, T. N. Tran, S. E. Braden, S. Wiseman, J. Danton,\\nE. M. Eliason, and M. S. Robinson. Flight Calibration of the LROC Narrow Angle Camera. Space\\nScience Reviews , 200(1-4):431‚Äì473, 2016.\\nL. Gaddis, J. Anderson, K. Becker, T. Becker, D. Cook, K. Edwards, E. Eliason, T. Hare, H. Kieffer, E. M.\\nLee, J. Mathews, L. Soderblom, T. Sucharski, J. Torson, A. McEwen, and M. Robinson. An Overview of\\nthe Integrated Software for Imaging Spectrometers (ISIS). In Lunar and Planetary Science Conference ,\\nLunar and Planetary Science Conference, page 387, March 1997.\\nFrederick R. Chromey. To Measure the Sky . Cambridge University Press, oct 2016.\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\\nimage segmentation. ArXiv e-prints , may 2015.\\nChen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to See in the Dark. In Proceedings of the\\nIEEE Computer Society Conference on Computer Vision and Pattern Recognition , pages 3291‚Äì3300,\\n2018. ISBN 9781538664209.\\nH. Riris, G. Neuman, J. Cavanaugh, X. Sun, P. Liiva, and M. Rodriguez. The Lunar Orbiter Laser\\nAltimeter (LOLA) on NASA‚Äôs Lunar Reconnaissance Orbiter (LRO) mission. In Naoto Kadowaki,\\neditor, International Conference on Space Optics ‚Äî ICSO 2010 , volume 10565, page 77. SPIE, nov\\n2017. ISBN 9781510616196.\\n5'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 3}, page_content='Baseline ISIS DestripeNet DestripeNet+PhotonNet\\nMode Camera L1 / Peak signal-to-noise ratio (PSNR) / Structural similarity index measure (SSIM)\\nNormal Left 8.12 / 29.89 / 0.56 3.79 / 35.23 / 0.79 3.67 / 35.63 / 0.80 1.08 /47.49 /0.98\\nRight 9.23 / 29.18 / 0.53 3.79 / 35.31 / 0.79 3.70 / 35.62 / 0.80 1.09 /47.62 /0.98\\nSummed Left 7.75 / 30.33 / 0.59 3.73 / 35.37 / 0.80 3.63 / 35.75 / 0.81 1.20 /46.76 /0.97\\nRight 9.19 / 29.29 / 0.54 3.79 / 35.39 / 0.80 3.72 / 35.65 / 0.81 1.25 /46.89 /0.97\\nTable 1: Test set performance of HORUS. We compare to a baseline approach, where simply the mean\\nmasked pixel value is subtracted from the raw image, and the current calibration routine (ISIS) (Humm\\net al., 2016; Gaddis et al., 1997). The average performance over all images is reported.\\nFigure 4: Ground truth veriÔ¨Åcation. The comparison of a non-HORUS sunlit and a HORUS shadowed\\nimage (contrast and brightness-adapted) in a temporarily shadowed region indicates that our denoising\\ndoes not remove or add any physical features. Non-HORUS image credit to LROC/ASU/GSFC.\\ndark calibration images and photon noise to re-scaled sunlit images and report the performance of HORUS\\non this dataset in Table 1. This table shows that HORUS signiÔ¨Åcantly outperforms the current calibration\\nroutine under all tested metrics for these images.\\nA key concern is whether our networks ‚Äúhallucinate‚Äù or remove physical features from the images. To\\nhelp investigate this, we plot a non-HORUS sunlit image of a Temporarily Shadowed Region (TSR) and\\ncompare this to a shadowed image with HORUS applied in Figure 4. Encouragingly, we Ô¨Ånd that HORUS\\ndoes not remove or add any incorrect physical features compared to the sunlit image.\\n4 Conclusion and Further Work\\nWe have shown that it is possible to signiÔ¨Åcantly enhance the quality of extremely low-light images of\\nPSRs on the Moon by using two physics-based deep neural networks to remove CCD and photon noise.\\nOur approach provides high-resolution, low-noise images which could help to directly detect water-ice.\\nWe have run HORUS over 20+ PSRs and future work will analyse these images for surface-ice related\\nsignals, as well as quantitatively assess the performance of HORUS using ground truth TSR images and\\ninvestigate joint training of the networks to further improve performance.\\n5 Broader impact\\nThis work could signiÔ¨Åcantly impact humanity‚Äôs plans for exploration of the lunar poles, helping to enable\\na sustainable, long term presence on the Moon and beyond. First, our work could directly inform robotic\\nscouting missions to the poles, enabling identiÔ¨Åcation of water(-ice)-related science targets and reducing\\nuncertainty when planning traverses into PSRs. Secondly, our work could support the planning and\\nexecution of the upcoming NASA Artemis missions, which will see the Ô¨Årst woman and the next man\\noperating around and potentially within lunar polar PSRs.\\nAcknowledgements. This work was completed as part of the 2020 NASA Frontier Development Lab\\n(FDL). The authors would like to thank Julie Stopar and Nick Estes for insightful comments on the\\nNAC instrument, Eugene D‚ÄôEon for advice on the ray tracing, and all of the FDL reviewers and partners\\n(Luxembourg Space Agency, Google Cloud, Intel AI and NVIDIA).\\nContributions. VTB and BM were involved in the conceptualization of this project. VTB, BM, ILF and\\nLR were involved in the methodology, software, validation, formal analysis, investigation, resources, data\\ncuration, writing, preparation, visualization. VTB, BM, ILF, LR, AZ, DW and MOM were involved in\\nthe review, administration, supervision. ILF, AZ, DW and the FDL management team were involved in\\nfunding acquisition. NS and EDE implemented the 3D ray-tracing.\\n4'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 1}, page_content='Figure 1: Performance of HORUS over 3 example PSRs, compared to the current calibration routine\\n(ISIS) (Humm et al., 2016; Gaddis et al., 1997). Colour bar shows the estimated surface signal Sfor the\\nDestripeNet+PhotonNet plot. Raw/ISIS image credits to LROC/ASU/GSFC.\\n2 Methods\\nThe LRO NAC takes 12-bit panchromatic optical images and consists of two nominally identical, 5064-\\npixel line-scan CCD cameras oriented perpendicular to the direction of Ô¨Çight of the satellite (Robinson\\net al., 2010). Each camera has two operating modes, ‚Äúregular‚Äù and ‚Äúsummed‚Äù, where the summed mode\\nsums adjacent pixels and is typically used to maximize the signal received over poorly lit regions of the\\nMoon. Over 10 years of operation the LRO NAC has captured over 2 million images, covering the entire\\nsurface multiple times over.\\nFigure 1 shows example raw NAC images captured over 3 example PSRs at the lunar south pole. Whilst\\nthey receive no direct sunlight, PSRs can be illuminated by indirect light scattered from their surroundings.\\nIn this extremely low-light setting NAC images are dominated by CCD noise and photon noise, which\\nis due to the inherent randomness of photons arriving at the CCD. Informed by the work of Humm et al.\\n(2016) and standard CCD theory (Chromey, 2016), in this work we assume that this noise can be described\\nby the following physical model,\\nI=S+Np+Nd+Nb+Nr, (1)\\nwhere Iis the raw image detector count, Sis the mean photon signal, Npis the stochastic photon noise,\\nwhich obeys a Poisson distribution with an expected value equal to S,Ndis stochastic dark current noise,\\nwhich is generated by thermally-activated electrons in the CCD and obeys a Poisson distribution, Nbis\\nthe dark bias, which is deterministic and due to a voltage offset which varies pixel-to-pixel, and Nris the\\nreadout noise. Of these noise sources, the photon noise and dark bias are considered the most dominant\\n(Humm et al., 2016) and are the focus of this study.\\nThe current calibration routine for the NAC images (ISIS, or Integrated Software for Imagers and Spec-\\ntrometers (Humm et al., 2016; Gaddis et al., 1997)) only attempts to remove the dark bias from each\\nimage, by subtracting a cyclic trend from its set of 60 masked pixels and a single library dark calibration\\nframe. Examples images after this calibration are shown in Figure 1: we observe that photon noise and a\\nsigniÔ¨Åcant amount of dark bias (residual stripes) are still present.\\nIn this work we remove both photon and dark bias noise from the images by using two deep neural\\nnetworks, collectively named HORUS. The Ô¨Årst network is called DestripeNet and it models the dark\\nbias. The input to the network is the camera metadata available at the time of image capture (listed in\\nFigure 2) and its output is a prediction of the dark bias frame. We train this network using a set of 100,000+\\ncalibration images as labels, taken on the non-illuminated side of the Moon, where negligible photon signal\\nis present. We use a convolutional decoder network design, shown in Figure 2 (top). Thus, this network\\nlearns a time-varying physics-based model of the CCD sensor.\\nThe second network is called PhotonNet and it models photon noise in the images. The input to the network\\nis the image after DestripeNet has been applied and its output is a prediction of the mean surface signal S.\\nThis network is trained using synthetically generated noisy images by taking 1 million randomly selected\\n2'),\n",
       " Document(metadata={'source': 'NeurIPS_ML4PS_2020_43.pdf', 'page': 2}, page_content='Figure 2: Physics-based HORUS denoising approach and noise model. Top: DestripeNet and PhotonNet\\nnetworks. DestripeNet uses a convolutional decoder design with 13 layers and 512 hidden channels to\\nmodel the dark bias frame Nb. PhotonNet uses a U-Net design with 4 down-sampling steps to remove\\nphoton noise Np. Both networks are trained using an L2 loss function. PhotonNet operates on images\\npatches, whilst DestripeNet predicts by image line. Bottom: noise model used in this work, showing an\\nexample dark calibration frame ( Nb). Image credits to LROC/ASU/GSFC.\\n40 60 80 100 120\\nSolar incidence angle / secondary incidence angle (degrees)0200400600800Number of NAC imagesTraining images\\n0.00.20.40.60.81.0\\nNormalised secondary illumination fluxShackleton\\nFaustini\\nCabeus\\nScott E\\n0 15 30 45 60 75\\nx (km)0\\n15\\n30\\n45\\n60y (km)\\n0 20 40 60 80\\nMean photon signal, S0.00.20.40.60.81.0Normalised count\\nFigure 3: 3D ray tracing and training distribution matching. Left: histogram of the secondary incidence\\nangle of scattered rays for 4 PSRs simulated using 3D ray tracing, compared to the distribution of solar\\nincidence angles in the training set for PhotonNet. Middle: A render of the secondary illumination radiance\\nfor the Shackleton crater PSR; here the crater rim is a strong source of scattered light. Right: distribution of\\nphoton counts over the re-scaled bright images used to train PhotonNet. Typically, PSRs have raw detector\\ncounts Iin the range 0-50 and we re-scale to this level.\\nbright, sunlit patches of the Moon, re-scaling them to low-lighting photon counts, adding Poisson noise to\\nthem, and then learning to remove this noise. We use a U-Net design (Ronneberger et al., 2015), similar to\\nother low-light denoising approaches (Chen et al., 2018).\\nAn important consideration is that the predictions of PhotonNet will be biased towards the distribution of\\nimages it has been trained on. Whilst it is trained on images with direct solar illumination, it is intended\\nfor use on images of PSRs, which are illuminated by secondary (backscattered) light only. In order to\\nmatch the training data to the test-time data as best we can, we carry out 3D ray tracing of 4 PSRs at the\\nsouth pole and plot the expected distribution of secondary illumination angle under typical illumination\\nconditions. We then match the distribution of solar incidence angles in our training images for PhotonNet\\nto this distribution. Ray tracing is performed using 30 m/pixel spatial resolution LRO Lunar Orbiter Laser\\nAltimeter elevation data (Riris et al., 2017) and a Lambertian bidirectional reÔ¨Çection function. Finally, to\\naccount for possible differences in the two cameras and two operating modes, we train separate networks\\nfor each possible conÔ¨Åguration.\\n3 Results and Discussion\\nHistograms of the incidence angle of scattered rays from ray tracing are shown in Figure 3. We Ô¨Ånd that\\nthese rays typically have incidence angles in the range 60 to 100 degrees (normal to the Moon‚Äôs sphere)\\nand our matched distribution of solar incidence angles in the training set is shown.\\nExample real-data test images of PSRs after training DestripeNet and PhotonNet are shown in Figure 1.\\nWe Ô¨Ånd that DestripeNet removes more of the dark bias ‚Äústripes‚Äù in the images compared to the current\\ncalibration routine, and PhotonNet signiÔ¨Åcantly improves the image quality, removing the ‚Äúgraininess‚Äù of\\nthe photon noise. We generate a test set of 100,000 unseen synthetic noisy images by adding samples of\\n3')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"Document\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
